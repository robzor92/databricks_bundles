{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9d92da",
   "metadata": {},
   "source": [
    "***Connect to Hopsworks Feature Store***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a82dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsfs\n",
    "conn = hsfs.connection(\n",
    "    host=\"020d3ad0-7247-11ee-8227-d9815d55065e.cloud.hopsworks.ai\",                                # DNS of your Feature Store instance\n",
    "    project=\"myproject\",                      # Name of your Hopsworks Feature Store project\n",
    "    hostname_verification=False,                     # Disable for self-signed certificates\n",
    "    api_key_value=\"clsslXKEqyCMAKNb.1As1OohStIij78WicPSkC1Zkumwj8vxHxoWoyZJ1EbtFArrd1oI2ZsuglgMNjfDl\"          # Feature store API key value \n",
    ")\n",
    "fs = conn.get_feature_store()           # Get the project's default feature store\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712ec16",
   "metadata": {},
   "source": [
    "***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from math import radians\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from hsfs.feature import Feature\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19660c0",
   "metadata": {},
   "source": [
    "***Data loading***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c8575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_cards_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/credit_cards.csv\")\n",
    "profiles_df = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/profiles.csv\", parse_dates=[\"birthdate\"])\n",
    "trans_df_raw = pd.read_csv(\"https://repo.hops.works/master/hopsworks-tutorials/data/card_fraud_data/transactions.csv\", parse_dates=[\"datetime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a4bfb",
   "metadata": {},
   "source": [
    "***Data preparation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df = trans_df_raw.merge(profiles_df, on=\"cc_num\", how=\"left\")\n",
    "trans_df_raw[\"age_at_transaction\"] = (age_df[\"datetime\"] - age_df[\"birthdate\"]) / np.timedelta64(1, \"Y\")\n",
    "\n",
    "card_expiry_df = trans_df_raw.merge(credit_cards_df, on=\"cc_num\", how=\"left\")\n",
    "card_expiry_df[\"expires\"] = pd.to_datetime(card_expiry_df[\"expires\"], format=\"%m/%y\")\n",
    "trans_df_raw[\"days_until_card_expires\"] = (card_expiry_df[\"expires\"] - card_expiry_df[\"datetime\"]) / np.timedelta64(1, \"D\")\n",
    "\n",
    "trans_df_raw.sort_values(\"datetime\", inplace=True)\n",
    "trans_df_raw[[\"longitude\", \"latitude\"]] = trans_df_raw[[\"longitude\", \"latitude\"]].applymap(radians)\n",
    "\n",
    "\n",
    "def haversine(long, lat):\n",
    "    \"\"\"Compute Haversine distance between each consecutive coordinate in (long, lat).\"\"\"\n",
    "\n",
    "    long_shifted = long.shift()\n",
    "    lat_shifted = lat.shift()\n",
    "    long_diff = long_shifted - long\n",
    "    lat_diff = lat_shifted - lat\n",
    "\n",
    "    a = np.sin(lat_diff/2.0)**2\n",
    "    b = np.cos(lat) * np.cos(lat_shifted) * np.sin(long_diff/2.0)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a + b))\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "trans_df_raw[\"loc_delta\"] = trans_df_raw.groupby(\"cc_num\") \\\n",
    "    .apply(lambda x : haversine(x[\"longitude\"], x[\"latitude\"])) \\\n",
    "    .reset_index(level=0, drop=True) \\\n",
    "    .fillna(0)\n",
    "\n",
    "window_len = \"4h\"\n",
    "cc_group = trans_df_raw[[\"cc_num\", \"amount\", \"datetime\"]].groupby(\"cc_num\").rolling(window_len, on=\"datetime\")\n",
    "\n",
    "df_4h_mavg = pd.DataFrame(cc_group.mean())\n",
    "df_4h_mavg.columns = [\"trans_volume_mavg\", \"datetime\"]\n",
    "df_4h_mavg = df_4h_mavg.reset_index(level=[\"cc_num\"])\n",
    "df_4h_mavg = df_4h_mavg.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_mavg = df_4h_mavg.sort_index()\n",
    "\n",
    "df_4h_std = pd.DataFrame(cc_group.mean())\n",
    "df_4h_std.columns = [\"trans_volume_mstd\", \"datetime\"]\n",
    "df_4h_std = df_4h_std.reset_index(level=[\"cc_num\"])\n",
    "df_4h_std = df_4h_std.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_std = df_4h_std.fillna(0)\n",
    "df_4h_std = df_4h_std.sort_index()\n",
    "\n",
    "window_aggs_df_raw = df_4h_std.merge(df_4h_mavg,left_index=True, right_index=True)\n",
    "\n",
    "df_4h_count = pd.DataFrame(cc_group.mean())\n",
    "df_4h_count.columns = [\"trans_freq\", \"datetime\"]\n",
    "df_4h_count = df_4h_count.reset_index(level=[\"cc_num\"])\n",
    "df_4h_count = df_4h_count.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_count = df_4h_count.sort_index()\n",
    "window_aggs_df_raw = window_aggs_df_raw.merge(df_4h_count,left_index=True, right_index=True)\n",
    "\n",
    "cc_group = trans_df_raw[[\"cc_num\", \"loc_delta\", \"datetime\"]].groupby(\"cc_num\").rolling(window_len, on=\"datetime\").mean()\n",
    "df_4h_loc_delta_mavg = pd.DataFrame(cc_group)\n",
    "df_4h_loc_delta_mavg.columns = [\"loc_delta_mavg\", \"datetime\"]\n",
    "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.reset_index(level=[\"cc_num\"])\n",
    "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.drop(columns=[\"cc_num\", \"datetime\"])\n",
    "df_4h_loc_delta_mavg = df_4h_loc_delta_mavg.sort_index()\n",
    "window_aggs_df_raw = window_aggs_df_raw.merge(df_4h_loc_delta_mavg,left_index=True, right_index=True)\n",
    "\n",
    "window_aggs_df_raw = window_aggs_df_raw.merge(trans_df_raw[[\"cc_num\", \"datetime\"]].sort_index(),left_index=True, right_index=True)\n",
    "window_aggs_df_raw.tail()\n",
    "\n",
    "trans_df_raw.datetime = trans_df_raw.datetime.values.astype(np.int64) // 10 ** 6\n",
    "window_aggs_df_raw.datetime = window_aggs_df_raw.datetime.values.astype(np.int64) // 10 ** 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0d7ba",
   "metadata": {},
   "source": [
    "***Feature group creation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Groups\n",
    "trans_fg = fs.get_or_create_feature_group(\n",
    "    name=\"transactions_fraud_batch_fg\",\n",
    "    version=1,\n",
    "    description=\"Transaction data\",\n",
    "    primary_key=[\"cc_num\"],\n",
    "    event_time=\"datetime\"\n",
    ")\n",
    "\n",
    "_, _ = trans_fg.insert(trans_df_raw)\n",
    "\n",
    "trans_feature_descriptions = [\n",
    "    {\"name\": \"tid\", \"description\": \"Transaction id\"},\n",
    "    {\"name\": \"datetime\", \"description\": \"Transaction time\"},\n",
    "    {\"name\": \"cc_num\", \"description\": \"Number of the credit card performing the transaction\"},\n",
    "    {\"name\": \"category\", \"description\": \"Expense category\"},\n",
    "    {\"name\": \"amount\", \"description\": \"Dollar amount of the transaction\"},\n",
    "    {\"name\": \"latitude\", \"description\": \"Transaction location latitude\"},\n",
    "    {\"name\": \"longitude\", \"description\": \"Transaction location longitude\"},\n",
    "    {\"name\": \"city\", \"description\": \"City in which the transaction was made\"},\n",
    "    {\"name\": \"country\", \"description\": \"Country in which the transaction was made\"},\n",
    "    {\"name\": \"fraud_label\", \"description\": \"Whether the transaction was fraudulent or not\"},\n",
    "    {\"name\": \"age_at_transaction\", \"description\": \"Age of the card holder when the transaction was made\"},\n",
    "    {\"name\": \"days_until_card_expires\", \"description\": \"Card validity days left when the transaction was made\"},\n",
    "    {\"name\": \"loc_delta\", \"description\": \"Haversine distance between this transaction location and the previous transaction location from the same card\"},\n",
    "]\n",
    "\n",
    "for dictionary in trans_feature_descriptions:\n",
    "    trans_fg.update_feature_description(dictionary[\"name\"],\n",
    "                                        dictionary[\"description\"])\n",
    "\n",
    "window_aggs_fg = fs.get_or_create_feature_group(\n",
    "    name=f\"transactions_{window_len}_aggs_fraud_batch_fg\",\n",
    "    version=1,\n",
    "    description=f\"Aggregate transaction data over {window_len} windows.\",\n",
    "    primary_key=[\"cc_num\"],\n",
    "    event_time=\"datetime\"\n",
    ")\n",
    "\n",
    "_, _ = window_aggs_fg.insert(window_aggs_df_raw)\n",
    "\n",
    "window_aggs_feature_descriptions = [\n",
    "    {\"name\": \"datetime\", \"description\": \"Transaction time\"},\n",
    "    {\"name\": \"cc_num\", \"description\": \"Number of the credit card performing the transaction\"},\n",
    "    {\"name\": \"loc_delta_mavg\", \"description\": \"Moving average of location difference between consecutive transactions from the same card\"},\n",
    "    {\"name\": \"trans_freq\", \"description\": \"Moving average of transaction frequency from the same card\"},\n",
    "    {\"name\": \"trans_volume_mavg\", \"description\": \"Moving average of transaction volume from the same card\"},\n",
    "    {\"name\": \"trans_volume_mstd\", \"description\": \"Moving standard deviation of transaction volume from the same card\"},\n",
    "]\n",
    "\n",
    "for dictionary in window_aggs_feature_descriptions:\n",
    "    window_aggs_fg.update_feature_description(dictionary[\"name\"],\n",
    "                                              dictionary[\"description\"])\n",
    "\n",
    "window_aggs_fg_fetched = fs.get_or_create_feature_group(name=f\"transactions_{window_len}_aggs_fraud_batch_fg\",\n",
    "                                                        version=1,)\n",
    "\n",
    "# Query\n",
    "trans_cols_select = [\"fraud_label\", \"category\", \"amount\",\n",
    "                     \"age_at_transaction\", \"days_until_card_expires\",\n",
    "                     \"loc_delta\"]\n",
    "window_aggs_cols_except = [\"cc_num\"]\n",
    "\n",
    "\n",
    "query = trans_fg.select(trans_cols_select) \\\n",
    "                .join(window_aggs_fg.select_except(window_aggs_cols_except))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd98790",
   "metadata": {},
   "source": [
    "***Feature view creation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation functions\n",
    "min_max_scaler_code_expected = '{\\n  \"module_imports\": \"\",\\n  \"transformer_code\": \"def min_max_scaler(value, min_value, max_value):\\\\n    if value is None:\\\\n        return None\\\\n    elif float(max_value - min_value) == float(0):\\\\n        return float(0)\\\\n    else:\\\\n        return float((value - min_value) / (max_value - min_value))\\\\n\"\\n}'\n",
    "label_encoder_code_expected = '{\"module_imports\": \"\", \"transformer_code\": \"# label encoder\\\\ndef label_encoder(value, value_to_index):\\\\n    # define a mapping of values to integers\\\\n    return value_to_index[value]\"}'\n",
    "\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "label_encoder = fs.get_transformation_function(name=\"label_encoder\")\n",
    "\n",
    "min_max_scaler_dict = min_max_scaler.to_dict()\n",
    "label_encoder_dict = label_encoder.to_dict()\n",
    "\n",
    "transformation_functions = {\n",
    "    \"category\": label_encoder,\n",
    "    \"amount\": min_max_scaler,\n",
    "    \"trans_volume_mavg\": min_max_scaler,\n",
    "    \"trans_volume_mstd\": min_max_scaler,\n",
    "    \"trans_freq\": min_max_scaler,\n",
    "    \"loc_delta\": min_max_scaler,\n",
    "    \"loc_delta_mavg\": min_max_scaler,\n",
    "    \"age_at_transaction\": min_max_scaler,\n",
    "    \"days_until_card_expires\": min_max_scaler,\n",
    "}\n",
    "\n",
    "# Feature View\n",
    "query = trans_fg.select(trans_cols_select) \\\n",
    "                .join(window_aggs_fg.select_except(window_aggs_cols_except))\n",
    "\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "        name='fraud_batch_fv',\n",
    "        version=1,\n",
    "        query=query,\n",
    "        labels=[\"fraud_label\"],\n",
    "        transformation_functions=transformation_functions\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc374af2",
   "metadata": {},
   "source": [
    "***Create training data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80100cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splits\n",
    "VALIDATION_SIZE = 0.2\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "td_version, td_job = feature_view.create_train_validation_test_split(\n",
    "    description = 'transactions fraud batch training dataset',\n",
    "    data_format = 'csv',\n",
    "    validation_size = VALIDATION_SIZE,\n",
    "    test_size = TEST_SIZE,\n",
    "    write_options = {'wait_for_job': True}\n",
    ")\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = feature_view.get_train_validation_test_split(1)\n",
    "\n",
    "# get_batch_data\n",
    "date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "# Retrieve part of training dataset using event time filter\n",
    "start_time = int(float(datetime.strptime(\"2022-03-01 00:00:00\", date_format).timestamp()) * 1000)\n",
    "end_time = int(float(datetime.strptime(\"2022-03-31 23:59:59\", date_format).timestamp()) * 1000)\n",
    "\n",
    "feature_view.init_batch_scoring(1)\n",
    "\n",
    "batch_data = feature_view.get_batch_data(start_time=start_time, end_time=end_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}